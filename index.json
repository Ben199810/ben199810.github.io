[{"content":"","date":"19 August 2025","externalUrl":null,"permalink":"/","section":"bing-wei's Blog","summary":"","title":"bing-wei's Blog","type":"page"},{"content":" 介紹 # Helm 是 Kubernetes 的一個套件管理工具，可以用來簡化應用程式的部署和管理。它使用稱為 Chart 的包裝格式來定義應用程式的各種資源，並提供了一個簡單的命令列介面來安裝、升級和管理這些應用程式。\n函數 # Helm 提供了一些內建函數，可以在 Chart 的模板中使用。這些函數可以用來處理字串、數字、日期等資料類型，並提供了一些常用的功能，可以幫助開發者更方便地編寫和維護 Helm Chart。\nquote # 將字符串使用雙引號包裹起來。\n⭐️ sqoute 表示單引號。\nuserName: User1 {{ .Values.userName | quote }} 得到結果: \u0026ldquo;User1\u0026rdquo;\n{{ .Values.userName | squote }} 得到結果: \u0026lsquo;User1\u0026rsquo;\nreplace # 字串替換。\n\u0026#34;I Am Henry VIII\u0026#34; | replace \u0026#34; \u0026#34; \u0026#34;-\u0026#34; 得到結果: I-Am-Henry-VIII\ndict # 傳遞 key 和 value 來創建一個 map。\n{{ $myDict := dict \u0026#34;key1\u0026#34; \u0026#34;value1\u0026#34; \u0026#34;key2\u0026#34; \u0026#34;value2\u0026#34; }} 得到結果:\n{ \u0026#34;key1\u0026#34;: \u0026#34;value1\u0026#34;, \u0026#34;key2\u0026#34;: \u0026#34;value2\u0026#34; } fromYaml # 將 YAML 字串轉換為 Object。\nfilePath: yamls/person.yaml\nname: Bob age: 25 hobbies: - hiking - fishing - cooking {{- $person := .Files.Get \u0026#34;yamls/person.yaml\u0026#34; | fromYaml }} 得到結果:\n{ \u0026#34;name\u0026#34;: \u0026#34;Bob\u0026#34;, \u0026#34;age\u0026#34;: 25, \u0026#34;hobbies\u0026#34;: [ \u0026#34;hiking\u0026#34;, \u0026#34;fishing\u0026#34;, \u0026#34;cooking\u0026#34; ] } merge, mergeOverwrite # ⚠️ 注意目標 dst 優先\ndst: default: default overwrite: me key: true src: overwrite: overwritten key: false {{- $newdict := merge .Values.dst .Values.src }} 得到結果:\nnewdict: default: default overwrite: me key: true 如果要以 src 為優先，可以使用 mergeOverwrite 函數。\n{{- $newdict := mergeOverwrite .Values.dst .Values.src }} 得到結果:\nnewdict: default: default overwrite: overwritten key: false omit # 類似 pick，用於從 map 中排除指定的 key。\nvolumeMounts: - name: my-volume mountPath: /data claimName: my-pvc {{- range $mount := .volumeMounts }} {{- $cleanMount := omit $mount \u0026#34;claimName\u0026#34; }} {{- end }} 得到結果:\nvolumeMounts: - name: my-volume mountPath: /data append # 在已有陣列中追加一個元素，建立一個新的陣列。\nmyArray: - element1 - element2 - element3 {{- $newArray := append .Values.myArray \u0026#34;newElement\u0026#34; }} 得到結果:\nnewArray: - element1 - element2 - element3 - newElement 參考資料 # 模板函数列表 ","date":"19 August 2025","externalUrl":null,"permalink":"/posts/helm/","section":"Posts","summary":"介紹 # Helm 是 Kubernetes 的一個套件管理工具，可以用來簡化應用程式的部署和管理。它使用稱為 Chart 的包裝格式來定義應用程式的各種資源，並提供了一個簡單的命令列介面來安裝、升級和管理這些應用程式。","title":"Helm","type":"posts"},{"content":"","date":"19 August 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"19 August 2025","externalUrl":null,"permalink":"/journals/","section":"Journals","summary":"","title":"Journals","type":"journals"},{"content":"","date":"19 August 2025","externalUrl":null,"permalink":"/tags/nginx/","section":"Tags","summary":"","title":"nginx","type":"tags"},{"content":"","date":"19 August 2025","externalUrl":null,"permalink":"/tags/redirect/","section":"Tags","summary":"","title":"redirect","type":"tags"},{"content":"","date":"19 August 2025","externalUrl":null,"permalink":"/tags/rewrite/","section":"Tags","summary":"","title":"rewrite","type":"tags"},{"content":" 前言 # 近期公司網站要下架舊的前端網頁服務，ingress 調整後端服務路由。\n新的服務網頁使用 nginx rewrite 網址路徑。在檢查過程中發現在瀏覽器頁面會發現有些前端靜態資源渲染不出來，呈現空白的頁面。\n後來使用 redirect 方式，問題解決了。\n所以想要記錄一下 rewrite 和 redirect 的差異。\nrewrite # nginx 的 rewrite 是一種用於修改請求 URL 的規則機制，常用於 URL 重寫、導向（redirect）、隱藏真實路徑或做 SEO 優化。\n基本語法 # regex: 正則表達式，用於匹配原始的請求路徑。 replacement: 要替換成的新路徑或新的 URL。 flag: 可選的標誌，控制 rewrite 行為的標誌，例如 permanent（301）、redirect（302）、break、last 等。 rewrite regex replacement [flag]; 參數 # last: 停止 rewrite，將請求重新在 server 或 location 區塊中尋找匹配（一般用於 location 區塊內）。 break: 停止 rewrite，但不再重新尋找匹配，直接執行後續配置。 redirect: 臨時重定向（302），告知瀏覽器請求新的 URL。 permanent: 永久重定向（301）。 last 跟 break 屬於內部重定向，不會改變瀏覽器的 URL。\n當用戶訪問 /old/path 時，將其重寫為 /new/path。\nlocation / { rewrite ^/old/(.*)$ /new/$1 last; } redirect 跟 permanent 屬於外部重定向，會改變瀏覽器的 URL。\n當用戶訪問 /old/path 時，將其重定向到 /new/path。\nlocation / { rewrite ^/old/(.*)$ /new/$1 redirect; } 問題反思 # 在使用 rewrite 的過程中，發現某些靜態資源無法正確加載，這可能是因為 rewrite 會在內部處理請求，而不會改變瀏覽器的 URL，導致某些資源的請求路徑不正確。相對而言，redirect 會直接告訴瀏覽器新的請求路徑，從而避免了這個問題。\n參考 # 什麼是 Nginx rewrite？轉址的機制，與 return 301 或 redirect 的差別在哪？ ","date":"19 August 2025","externalUrl":null,"permalink":"/journals/nginx/rewrite-vs-redirect/","section":"Journals","summary":"前言 # 近期公司網站要下架舊的前端網頁服務，ingress 調整後端服務路由。","title":"Rewrite vs Redirect","type":"journals"},{"content":"","date":"19 August 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"13 August 2025","externalUrl":null,"permalink":"/tags/job/","section":"Tags","summary":"","title":"job","type":"tags"},{"content":"","date":"13 August 2025","externalUrl":null,"permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"kubernetes","type":"tags"},{"content":" 介紹 # Job 表示一次性的任務，執行完成後就會停止。\nJob 可以建立一個或多個 Pod，並將繼續重試 Pod 的執行，直到指定數量的 Pod 成功終止。\n範例 # 在系統中部署新的服務時，可以使用 Job 透過 command line 工具或腳本來執行一次性的任務，例如資料庫遷移、批次處理等。\nbackoffLimit 指定 Pod 失敗後重試的次數。如果重試次數超過此限制，Job 將被標記為失敗。 restartPolicy Pod 的容器可能因為多種不同的原因失效，\u0026ldquo;OnFailure\u0026rdquo; 使 Pod 留在目前的節點上，但容器會重新運行。 \u0026ldquo;Never\u0026rdquo;，當 Pod 失敗時，Job 控制器會啟動新的 Pod。 只能使用 \u0026ldquo;Never\u0026rdquo; 或 \u0026ldquo;OnFailure\u0026rdquo;。 activeDeadlineSeconds 指定 Job 的最大執行時間（以秒為單位）。如果 Job 在此時間內未完成，則會被終止。 activeDeadlineSeconds 的優先權高於 backoffLimit。 ttlSecondsAfterFinished Job 完成後預設是不會被清理的，可以設定此參數來指定 Job 完成後保留的時間（以秒為單位）。當時間到達後，Job 將被自動清理。 apiVersion: batch/v1 kind: Job metadata: name: example-job spec: ttlSecondsAfterFinished: 60 activeDeadlineSeconds: 30 template: spec: containers: - name: example image: example-image command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo Hello, Kubernetes! \u0026amp;\u0026amp; sleep 30\u0026#34;] restartPolicy: Never backoffLimit: 4 參考資料 # Kubernetes 文檔/概念/工作負載/工作負載管理/Job ","date":"13 August 2025","externalUrl":null,"permalink":"/posts/k8s/job/","section":"Posts","summary":"介紹 # Job 表示一次性的任務，執行完成後就會停止。","title":"Kubernetes Job","type":"posts"},{"content":"","date":"18 July 2025","externalUrl":null,"permalink":"/tags/datadog/","section":"Tags","summary":"","title":"datadog","type":"tags"},{"content":" 事件過程 # 近期公司同仁反應 Datadog 停止收集 Kafka 的 metrics，因此進行調查的過程紀錄下來。未來若有同樣的問題，可以參考這篇文章。\n架構圖 # 先來了解 Kafka Metrics 是如何被 Datadog 收集的。\ndatadog agent 會部署在 Kubernetes 環境中，且在 node 中運行。它會收集 Pod 服務的 metrics，也會收集 Kafka 的 metrics。\n要注意的是 Kafka 的 VM 並沒有部署 Datadog Agent，而是透過 Kubernetes 的 Datadog Agent 來收集 Kafka 的 metrics。\n這麼做的原因是 Datadog Agent 收費是會依照使用的 Agent 數量計算的，可以減少費用的產生。\nflowchart LR subgraph kubernetes subgraph node datadog[Datadog Agent] pod[Pod] end end subgraph GCE kafka[Kafka] end datadog --\u003e|收集| kafka datadog --\u003e|收集| pod 問題調查 # 確認網路連線是否異常\n首先確認 Datadog Agent 是否能夠連線到 Kafka 的 metrics endpoint。要怎麼確認 kafka 的 metrics endpoint 是什麼呢？\n可以在 Kafka 的配置檔中找到，在配置監控系統時，如果採用 JMX Exporter 的方式，則可以在 /etc/systemd/system/kafka.service 中找到。\n-Dcom.sun.management.jmxremote.port 這個參數指定了 JVM 的 JMX 監控端口，通常是 9134。\n-Dcom.sun.management.jmxremote.rmi.port 這個參數指定了 RMI (遠端方法呼叫) 的端口。預設情況下，JMX 會動態選擇一個 RMI 端口，在雲端或防火牆環境下，動態端口會導致難以開放防火牆規則。\n將 jmxremote.port 和 jmxremote.rmi.port 設定相同，可確保 JMX 監控只啟用此連接埠（此處為 9134）例，防火牆設定和安全監控管。\n[Service] Environment=\u0026#34;KAFKA_JMX_OPTS=-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=9134 -Djava.rmi.server.hostname=10.106.10.151\u0026#34; 檢查 VM 監控端口是否開啟\nnetstat -an | grep 9134 # 或 lsof -i :9134 檢查 Kubernetes 到 Kafka 的連線是否異常\n在 Kubernetes 環境中，建立一個 Pod，並使用 curl 或 telnet 測試連線到 Kafka 的 JMX 端口。\napiVersion: v1 kind: Pod metadata: name: test-kafka-JMX-connection spec: containers: - name: test-container image: maven:3.8-openjdk-11 command: - /bin/sh - -c args: - | echo \u0026#34;Downloading jmxterm...\u0026#34; curl -L -s -o /tmp/jmxterm.jar https://github.com/jiaqi/jmxterm/releases/download/v1.0.2/jmxterm-1.0.2-uber.jar; echo \u0026#34;Testing JMX connection to Kafka...\u0026#34; get -b kafka.server:type=KafkaServer,name=BrokerState Value | java -jar /tmp/jmxterm.jar -n -l ${KAFKA_JMX_HOST}:9134; echo \u0026#34;JMX connection test completed.\u0026#34; resources: limits: memory: \u0026#34;512Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; requests: memory: \u0026#34;256Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; 確認 Datadog Agent 的配置\n確認 Datadog Agent 的配置檔中是否有正確設定 Kafka 的 JMX 端口。通常在 /etc/datadog-agent/conf.d/kafka.d/conf.yaml 中。\ninit_config: {} instances: - host: \u0026lt;kafka-host\u0026gt; port: 9134 tags: - kafka Datadog Support 建議\nSupport 團隊建議可以使用 flare 傳遞資訊到案件單上，有助於排查問題。\n在 kafka integration 中，設置 checke runner (cluster_check: true) 。因此可以 flare cluster agent 的資訊。\nkubectl exec -it \u0026lt;NAMESPACE\u0026gt; -it \u0026lt;CLUSTER_POD_NAME\u0026gt; -- agent flare \u0026lt;案件編號\u0026gt; 檢查 cluster check runners。\nkubectl exec -n \u0026lt;NAMESPACE\u0026gt; -it \u0026lt;CLUSTER_POD_NAME\u0026gt; -- agent clusterchecks Datadog Support 回饋\n在提供上述的資訊之後，Support 團隊回覆在 flare (status.log) 中發現了 kafka instance 檢查的狀態。\n========= JMX Fetch ========= Information ================== runtime_version : 11.0.23 version : 0.49.1 Initialized checks ================== kafka - instance_name: kafka-02 metric_count: 346 service_check_count: 0 message: \u0026lt;no value\u0026gt; status: OK - instance_name: kafka-01 metric_count: 350 service_check_count: 0 message: Number of returned metrics is too high for instance: kafka-01. Please read http://docs.datadoghq.com/integrations/java/ or get in touch with Datadog Support for more details. Truncating to 350 metrics. status: WARNING - instance_name: kafka-03 metric_count: 350 service_check_count: 0 message: Number of returned metrics is too high for instance: kafka-03. Please read http://docs.datadoghq.com/integrations/java/ or get in touch with Datadog Support for more details. Truncating to 350 metrics. status: WARNING kafka-01 和 kafka-03 出現以下警告訊息：\nNumber of returned metrics is too high for instance: kafka-03. Please read http://docs.datadoghq.com/integrations/java/ or get in touch with Datadog Support for more details. Truncating to 350 metrics. Support 團隊建議由於傳回的指標數量太高，即高於預設值 350。因此，要解決此問題，您可以新增 max_returned_metrics 參數並將值設為高於 350。\n修正範例與驗證 # 在 values.yaml 中我們需要新增 max_returned_metrics 參數，並將其值設為需要的值且大於預設值。\nclusterAgent: confd: kafka.yaml: |- cluster_check: true init_config: is_jmx: true collect_default_metrics: true new_gc_metrics: true instances: - host: \u0026lt;kafka-host\u0026gt; port: 9134 name: kafka-01 max_returned_metrics: 500 # 新增此行 然後重新部署 Datadog Agent。\n使用 agent status 檢查 Kafka 的 metrics count 是否已經超過 350 且 status 從 WARNING 變為 OK。\n因為 agent 有很多台，所以使用腳本的方式去找到收集 Kafka VM Metrics 的 agent。\n#!/bin/bash echo \u0026#34;Found pods:\u0026#34; kubectl get pods -n datadog -o custom-columns=NAME:.metadata.name --no-headers | grep -v cluster read -p \u0026#34;搜尋關鍵字：\u0026#34; SEARCH_KEYWORD PODS=$(kubectl get pods -n datadog -o custom-columns=NAME:.metadata.name --no-headers | grep -v cluster) IFS=$\u0026#39;\\n\u0026#39; read -d \u0026#39;\u0026#39; -r -a POD_ARRAY \u0026lt;\u0026lt;\u0026lt; \u0026#34;$PODS\u0026#34; for POD in \u0026#34;${POD_ARRAY[@]}\u0026#34;; do echo \u0026#34;Checking status of pod: $POD\u0026#34; kubectl exec -n datadog $POD -c agent -- agent status | grep \u0026#34;$SEARCH_KEYWORD\u0026#34; if [ $? -ne 0 ]; then echo \u0026#34;No kafka-common found in $POD\u0026#34; fi echo \u0026#34;----------------------------------------\u0026#34; done echo \u0026#34;Finished checking all pods.\u0026#34; 找到對應的 agent 之後，使用以下指令檢查 Kafka 的 metrics count。\n可以看到 kafka 的 metrics count 已經超過 350 且 status 從 WARNING 變為 OK。\n========= JMX Fetch ========= Information ================== runtime_version : 11.0.23 version : 0.49.1 Initialized checks ================== kafka - instance_name: kafka-02 metric_count: 346 service_check_count: 0 message: \u0026lt;no value\u0026gt; status: OK - instance_name: kafka-01 metric_count: 476 service_check_count: 0 message: \u0026lt;no value\u0026gt; status: OK - instance_name: kafka-03 metric_count: 443 service_check_count: 0 message: \u0026lt;no value\u0026gt; status: OK Failed checks ============= no checks 問題反思 # 這次問題的根本原因是 Kafka 的 metrics 數量超過了 Datadog 的預設限制，導致部分 metrics 無法被收集。\n如果不提高 max_returned_metrics 的上限值，有方法可以降低 metric_count 的數量嗎？\nsupport 提供以下的方式減少 metric_count 的數量：\nmetric_count 值是每個 instance 的 Datadog 指標總數。\n對於 Kafka integration，Datadog 指標是藉助此文件 metrics.yaml （ 連結 ） 生成的，如果集成找到具有匹配的 domain、bean 和 attribute 的 jmx 指標，它將生成 Datadog 指標。\n如果您想要減少 metric_count 值，您可以將此參數變更為 collect_default_metrics 為 false ，這樣就不會使用 metrics.yaml ，然後建立您自己的配置以專門匹配任何 jmx 指標以從此範例（ 連結 ） 產生 Datdog 指標。\n參考資料 # 聊聊 Kafka 如何基於 JMX 監控 ","date":"18 July 2025","externalUrl":null,"permalink":"/journals/datadog/datadog-stopped-collecting-kafka-metrics/","section":"Journals","summary":"事件過程 # 近期公司同仁反應 Datadog 停止收集 Kafka 的 metrics，因此進行調查的過程紀錄下來。未來若有同樣的問題，可以參考這篇文章。","title":"Datadog Stopped Collecting Kafka Metrics","type":"journals"},{"content":"","date":"18 July 2025","externalUrl":null,"permalink":"/tags/kafka/","section":"Tags","summary":"","title":"kafka","type":"tags"},{"content":"","date":"18 July 2025","externalUrl":null,"permalink":"/tags/metrics/","section":"Tags","summary":"","title":"metrics","type":"tags"},{"content":"","date":"14 July 2025","externalUrl":null,"permalink":"/tags/lint/","section":"Tags","summary":"","title":"lint","type":"tags"},{"content":"","date":"14 July 2025","externalUrl":null,"permalink":"/tags/yaml/","section":"Tags","summary":"","title":"yaml","type":"tags"},{"content":" 介紹 # yamllint 不僅檢查語法的有效性，還檢查諸如鍵重複之類的異常情況以及諸如行長度、尾隨空格、縮排等外觀問題。\n有時候尾隨空格可能會導致 YAML 文件解析錯誤，這在處理大型配置文件時尤其重要。\n對於時常維護大量 YAML 文件的開發者來說是一個非常有用的工具。\n安裝 # brew install yamllint 使用 # 當前的目錄下對所有 YAML 文件進行檢查。\n如果有 .yamllint 配置文件，預設會自動載入文件的配置。\nyamllint . 預設 # yamllint 有一些預設檔案配置。\n--- yaml-files: # 這個配置指定了要檢查的 YAML 文件類型。 - \u0026#39;*.yaml\u0026#39; - \u0026#39;*.yml\u0026#39; - \u0026#39;.yamllint\u0026#39; rules: anchors: enable braces: enable brackets: enable colons: enable commas: enable comments: level: warning comments-indentation: level: warning document-end: disable document-start: level: warning empty-lines: enable empty-values: disable float-values: disable hyphens: enable indentation: enable key-duplicates: enable key-ordering: disable line-length: enable new-line-at-end-of-file: enable new-lines: enable octal-values: disable quoted-strings: disable trailing-spaces: enable truthy: level: warning yamllint 還有另一個預定義的配置叫 relaxed，顧名思義就是放寬檢查，寬容性更高。\nyamllint -d relaxed file.yml 配置 # 如果需要自定義配置，可以不必全部的配置都重新定義一次。使用 extends 關鍵字擴展預設配置。\nextends: default line-length: max: 80 level: warning 忽略檢查 # 如果像 helm template 雖然是 YAML 文件，但是使用 go template 語法，這時候可以使用 ignore 來忽略檢查。\nignore: - *.template.yaml 參數規則 # anchers：檢查錨點和別名的使用情況。 braces：控制 Flow Mapping 中大括號的使用。 brackets：控制 Flow Sequence 中方括號的使用。 document-start：檢查文件開頭是否有 ---。 rules: anchers: # 回報重複的錨點跟未聲明的錨點。 forbid-undeclared-aliases: true # 未聲明 forbid-duplicated-anchors: false # 重複 forbid-unused-anchors: false # 未使用 braces: # 控制 Flow Mapping (內行型的鍵值對) 中大括號的使用。 forbid: false # 是否禁止使用 Flow Mapping min-spaces-inside: 0 # 大括號內部的最小空格數 max-spaces-inside: 0 # 大括號內部的最大空格數 min-spaces-inside-empty: -1 # 空大括號內部的最小空格數 max-spaces-inside-empty: -1 # 空大括號內部的最大空格數 brackets: # 控制 Flow Sequence (內行型的列表) 中方括號 forbid: false # 是否禁止使用 Flow Sequence min-spaces-inside: 0 max-spaces-inside: 0 min-spaces-inside-empty: -1 max-spaces-inside-empty: -1 document-start: # 檢查文件開頭是否有 `---`。 present: true # 是否需要 `---` 開頭 empty-lines：檢查空行的使用情況。 為了規範團隊的代碼風格，可以設定空行的數量。很常有人會使用過多的空行來分隔代碼塊，這樣會導致代碼不易閱讀。\nrules: empty-lines: max: 1 # 允許的最大空行數 example\npass\n- foo: - 1 - 2 - bar: [3, 4] fail\n- foo: - 1 - 2 - bar: [3, 4] indentation：控制縮排的使用情況。\nYaml 的文件縮進非常靈活，但為了保持一致性，通常會使用兩個空格或四個空格來縮排。為了更好的規範團隊的代碼風格，可以設定縮排的空格數量。\nindent-sequences 決定對於序列 (array) 的縮排方式，提供四種選擇：true、false、consistent 和 whatever。\nconsistent 要求所有的序列都使用相同的縮排方式，可以全部都縮排或者全部都不縮排。\nwhatever 則允許序列的縮排方式不一致，這樣可以更靈活地處理不同的情況。\nrules: indentation: spaces: 2 # 縮排使用的空格數量 indent-sequences: true example pass\nhistory: - name: Unix date: 1969 - name: Linux date: 1991 進階用法 # 錨點和別名 # YAML 中的錨點 (anchors) 和別名 (aliases) 用於重複使用相同的值或結構，這樣可以減少重複代碼並提高可讀性。\n以下面的 YAML 為例：\n先使用 \u0026amp; 定義一個錨點，然後使用 * 來引用這個錨點。\ndefault_config 錨點設定了 key1 和 key2 會重複使用到的設定值。接著使用 \u0026lt;\u0026lt; 來合併這個錨點到其他配置中。\n\u0026amp;default_config: key1: value1 key2: value2 my_config: \u0026lt;\u0026lt;: *default_config # 使用別名來引用錨點 key3: value3 another_config: \u0026lt;\u0026lt;: *default_config # 再次使用同一個錨點 key4: value4 Flow Mapping # Flow Mapping 是 YAML 中一種使用大括號 {} 來表示鍵值對的方式。這種方式通常用於簡化表示多個鍵值對的情況。\n一般來說，最常見、易讀的寫法是 Block Mapping，即使用縮排的方式來表示層級關係。\n# Block Mapping key1: subkey1: value1 subkey2: value2 # Flow Mapping key1: {subkey1: value1, subkey2: value2} 參考資料 # yamllint 文檔 用 YAML anchors \u0026amp; aliases 寫出更好維護的 docker compose file ","date":"14 July 2025","externalUrl":null,"permalink":"/posts/tool/yamllint/","section":"Posts","summary":"介紹 # yamllint 不僅檢查語法的有效性，還檢查諸如鍵重複之類的異常情況以及諸如行長度、尾隨空格、縮排等外觀問題。","title":"Yamllint","type":"posts"},{"content":"","date":"2 July 2025","externalUrl":null,"permalink":"/tags/autoscaling/","section":"Tags","summary":"","title":"autoscaling","type":"tags"},{"content":"","date":"2 July 2025","externalUrl":null,"permalink":"/tags/helm/","section":"Tags","summary":"","title":"helm","type":"tags"},{"content":"最近在升級 helm release 時遇到了一個錯誤，提示 \u0026ldquo;no matches for kind HorizontalPodAutoscaler in version autoscaling/v2beta2\u0026rdquo;。\n原因是因為 Kubernetes 的 API 版本變更，導致舊的資源定義不再匹配新的 API 版本。\n解決方法 # 因為是正式站環境的 log 服務，log 的收集對 RD 在排查問題時非常重要。如果刪除後再重新部署，會導致 log 的丟失。\nHelm3 支援一款插件工具 mapkubeapis，可以將有使用已棄用或已刪除的 API 的 Helm Release metadata 更新為有受支援的 API。\nhelm mapkubeapis -n kube_namespace releaseName 參考資料 # Helm upgrade failure due to deprecated or removed Kubernetes API ","date":"2 July 2025","externalUrl":null,"permalink":"/journals/k8s/kube-object-no-match-for-kind-version/","section":"Journals","summary":"最近在升級 helm release 時遇到了一個錯誤，提示 \u0026ldquo;no matches for kind HorizontalPodAutoscaler in version autoscaling/v2beta2\u0026rdquo;。","title":"Kube Object No Match for Kind Version","type":"journals"},{"content":"","date":"2 July 2025","externalUrl":null,"permalink":"/tags/mapkubeapis/","section":"Tags","summary":"","title":"mapkubeapis","type":"tags"},{"content":" 介紹 # PersistentVolume (PV) 是 Kubernetes 中的一種資源，用於提供持久化存儲。它是集群級別的存儲資源，與 Pod 的生命週期無關。PV 可以被多個 Pod 共享，並且可以在 Pod 重啟或重新調度時保持數據不變。\n管理員可以事先創建 PV，並將其與特定的存儲系統（如 NFS、iSCSI、雲存儲等）綁定，或使用動態存儲類別（StorageClass）來自動創建 PV。\nPersistentVolumeClaim (PVC) 是用戶請求存儲的方式。概念與 Pod 請求資源類似。用戶可以創建 PVC，指定所需的存儲大小和存取模式 (如 ReadWriteOnce、ReadOnlyMany 等)。Kubernetes 會根據 PVC 的要求來匹配合適的 PV。\n存取模式 # ReadWriteOnce (RWO)：卷只能被單個節點以讀寫方式掛載。 ReadOnlyMany (ROX)：卷可以被多個節點以只讀方式掛載。 ReadWriteMany (RWX)：卷可以被多個節點以讀寫方式掛載。 PersistentVolume 與 PersistentVolumeClaim # graph pod(Pod) volume(Volume) PVC(PersistentVolumeClaim) PV(PersistentVolume) storageClass(StorageClass) storageProvider(StorageProvider) storage(Physical Storage) pod --\u003e|使用| volume --\u003e PVC PVC --\u003e|靜態綁定| PV PVC --\u003e|動態綁定| storageClass PV --\u003e|提供存儲| storage storageClass --\u003e|配置存儲| storageProvider --\u003e|實際存儲| storage 如果 PVC storageClassName 屬性為 \u0026quot;\u0026quot;，則表示使用靜態綁定。這意味著 PVC 自身禁止使用動態製備的卷。\n參考 # [Kubernetes / K8s] PV/ PVC 儲存大小事交給PV/PVC管理 持久卷 ","date":"2 July 2025","externalUrl":null,"permalink":"/posts/k8s/storage/","section":"Posts","summary":"介紹 # PersistentVolume (PV) 是 Kubernetes 中的一種資源，用於提供持久化存儲。它是集群級別的存儲資源，與 Pod 的生命週期無關。PV 可以被多個 Pod 共享，並且可以在 Pod 重啟或重新調度時保持數據不變。","title":"PersistentVolume","type":"posts"},{"content":"","date":"2 July 2025","externalUrl":null,"permalink":"/tags/storage/","section":"Tags","summary":"","title":"storage","type":"tags"},{"content":" 介紹 # 每個 StorageClass 都包含 provisioner、parameters、reclaimPolicy 這些字段會在 StorageClass 需要動態配置 PersistentVolume 以滿足 PersistentVolumeClaim (PVC) 使用到。\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: low-latency provisioner: csi-driver.example-vendor.example reclaimPolicy: Retain # 默认值是 Delete allowVolumeExpansion: true mountOptions: - discard # 这可能会在块存储层启用 UNMAP/TRIM volumeBindingMode: WaitForFirstConsumer parameters: guaranteedReadWriteLatency: \u0026#34;true\u0026#34; # 这是服务提供商特定的 磁碟區插件 # 每個 StorageClass 都會有 provisioner ，用來決定使用哪個磁碟區插件。(必要)\n卷插件 內置製備器 配置範例 AzureFile ✓ Azure File CephFS - - FC - - FlexVolume - - iSCSI - - Local - Local NFS - NFS PortworxVolume ✓ Portworx Volume RBD ✓ Ceph RBD VsphereVolume ✓ vSphere 回收策略 # StorageClass 動態建立的 PersistentVolume 會在 reclaimPolicy 指定回收策略，可以是 Delete 或 Retain 。預設是 Delete 。\n擴展 # PersistentVolume 可以配置為可擴充，允許透過 PVC 物件來調整磁碟區大小，申請一個新的、更大的儲存容量。當 allowVolumeExpansion 定義為 true 時，下列類型的磁碟區支援擴充。\n卷類型 卷擴展的Kubernetes 版本要求 Azure File 1.11 CSI 1.24 FlexVolume 1.13 Portworx 1.11 rbd 1.11 掛載選項 # 由 StorageClass 動態建立的 PersistentVolume 將使用類別中 mountOptions指定的掛載選項。\n如果磁碟區插件不支援掛載選項，卻指定了掛載選項，則製備操作會失敗。 掛載選項在 StorageClass 和 PV 上都不會做驗證。如果其中一個掛載選項無效，那麼這個PV 掛載作業就會失敗。\n參考 # 動態卷製備 ","date":"1 July 2025","externalUrl":null,"permalink":"/posts/k8s/storageclass/","section":"Posts","summary":"介紹 # 每個 StorageClass 都包含 provisioner、parameters、reclaimPolicy 這些字段會在 StorageClass 需要動態配置 PersistentVolume 以滿足 PersistentVolumeClaim (PVC) 使用到。","title":"StorageClass","type":"posts"},{"content":"","date":"24 September 2023","externalUrl":null,"permalink":"/tags/m1/","section":"Tags","summary":"","title":"m1","type":"tags"},{"content":"","date":"24 September 2023","externalUrl":null,"permalink":"/tags/m2/","section":"Tags","summary":"","title":"m2","type":"tags"},{"content":"","date":"24 September 2023","externalUrl":null,"permalink":"/tags/mac/","section":"Tags","summary":"","title":"mac","type":"tags"},{"content":"","date":"24 September 2023","externalUrl":null,"permalink":"/tags/minikube/","section":"Tags","summary":"","title":"minikube","type":"tags"},{"content":" 介紹 # minikube 是一個輕量級的 Kubernetes 環境，主要用於本地開發和測試。它可以在本地機器上啟動一個單節點的 Kubernetes 集群，讓開發者可以快速地部署和測試應用程式。\nminikube 支援多種虛擬化技術，包括 VirtualBox、VMware、KVM、Hyper-V 等等。它可以在 Windows、macOS 和 Linux 上運行。\nQemu # Qemu 是一個開源的虛擬化軟體，可以模擬多種硬體架構，包括 x86、ARM、MIPS 等等。它可以用於虛擬化和模擬不同的作業系統和應用程式。\nQemu 的主要功能包括：\n使用者程式模擬：QEMU 能夠將為一個平台編譯的二進位檔案運行在另一個不同的平台。 系統虛擬化模擬：QEMU 能夠模擬一個完整的系統虛擬機，該虛擬機有自己的虛擬CPU、晶片組、虛擬記憶體以及各種虛擬外部設備，能夠為虛擬機中運行的作業系統和應用軟體呈現出與實體電腦完全一致的硬體視圖。QEMU能夠模擬 x86、ARM、MIPS、PPC 等多個平台。 Qemu 在 macOS 上的使用主要是為了支援 M 系列晶片（如 M1、M2）的虛擬化，因為這些晶片使用 ARM 架構，而許多傳統的虛擬化技術（如 VirtualBox）並不支援 ARM 架構。\n安裝 # 在 macOS 上安裝 minikube 和 Qemu，可以使用 Homebrew 來簡化安裝過程。 首先，確保你已經安裝了 Homebrew。如果還沒有安裝，可以參考 Homebrew 官方網站 進行安裝。\nbrew install minikube brew install qemu 接著，為了讓 minikube 使用 Qemu 作為虛擬化驅動程式，我們需要安裝 socket_vmnet。這是一個用於 macOS 的 Qemu 虛擬化網路驅動程式。\nbrew install socket_vmnet brew tap homebrew/services HOMEBREW=$(which brew) \u0026amp;\u0026amp; sudo ${HOMEBREW} services start socket_vmnet 啟動 Minikube # 在安裝完成後，可以使用以下命令啟動 minikube，並指定使用 Qemu 作為虛擬化驅動程式。\nminikube start --driver qemu --network socket_vmnet 參考資料 # How to Setup Minikube on MAC M1/M2 ","date":"24 September 2023","externalUrl":null,"permalink":"/posts/minikube/","section":"Posts","summary":"介紹 # minikube 是一個輕量級的 Kubernetes 環境，主要用於本地開發和測試。它可以在本地機器上啟動一個單節點的 Kubernetes 集群，讓開發者可以快速地部署和測試應用程式。","title":"Minikube 環境與安裝 (Mac M1/M2)","type":"posts"},{"content":"","date":"22 September 2023","externalUrl":null,"permalink":"/tags/cicd/","section":"Tags","summary":"","title":"cicd","type":"tags"},{"content":"","date":"22 September 2023","externalUrl":null,"permalink":"/tags/gitlab/","section":"Tags","summary":"","title":"gitlab","type":"tags"},{"content":" 介紹 # GitLab 是一個開源的 DevOps 平台，提供了版本控制、CI/CD、代碼審查等功能。它的 CI/CD 功能可以幫助開發者自動化軟體開發流程，提高開發效率和質量。\nGitLab 的 CI/CD 功能基於 GitLab Runner，允許開發者定義自動化的工作流程，從代碼提交到部署生產環境都可以自動化完成。\nGitLab CI/CD 的配置文件是 .gitlab-ci.yml，這個文件定義了 CI/CD 的工作流程，包括各個階段（stages）、任務（jobs）和執行的腳本（scripts）。開發者可以根據自己的需求定義不同的工作流程，並且可以在不同的階段中執行不同的任務。\n基本配置 # 以下是一個簡單的 .gitlab-ci.yml 配置範例，包含了三個階段：test、build 和 deploy。\nstages: - test - build - deploy test: stage: test script: # 定義 test 階段的指令 - echo \u0026#34;Running tests\u0026#34; # 執行單元測試、程式碼品質檢查等 - npm test - npm run lint build: stage: build script: # 定義 build 階段的指令 - echo \u0026#34;Building the application\u0026#34; - docker build -t $IMAGE_NAME:$TAG . # 只有當測試通過後才執行 build needs: [\u0026#34;test\u0026#34;] deploy: stage: deploy script: # 定義 deploy 階段的指令 - echo \u0026#34;Deploying to production\u0026#34; # 在實際情況下，這裡可以是部署到 Kubernetes、AWS、GCP 等的相應指令 # 也可以使用 Helm 進行部署 # 只有當 build 成功後才執行 deploy needs: [\u0026#34;build\u0026#34;] 進階配置 # 在生產環境中，通常需要更複雜的配置，例如：配置環境變數、建構 mysql 資料庫、redis 等服務。\nstages: - test - build - deploy test: stage: test services: - name: mysql:5.7 alias: db - redis:latest alias: redis variables: MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD} MYSQL_DATABASE: ${MYSQL_DATABASE} REDIS_PASSWORD: ${REDIS_PASSWORD} script: - echo \u0026#34;Running tests\u0026#34; - npm test - npm run lint build: stage: build script: - echo \u0026#34;Building the application\u0026#34; - docker build -t $IMAGE_NAME:$TAG . needs: [\u0026#34;test\u0026#34;] deploy: stage: deploy script: - echo \u0026#34;Deploying to production\u0026#34; needs: [\u0026#34;build\u0026#34;] parallel:matrix # ⚠️ 對於 runner 的配置，必須要有多台 runner 或者配置單台 runner 配置支援同時執行多個任務。\n⚠️ 矩陣的排列數不能超過 200\nparallel 可以在單一的 pipeline 中同時執行多個任務，這對於需要在多個環境或配置下運行相同任務的情況非常有用。\n如果需要在不同的 Node.js 版本和環境下運行測試，可以使用 parallel:matrix 來定義一個矩陣，這樣可以在不同的配置下同時運行相同的任務。\ntest: stage: test script: - echo \u0026#34;Running tests\u0026#34; parallel: matrix: - NODE_VERSION: [10, 12, 14] - ENV: [development, production] 參考 # Parallel ","date":"22 September 2023","externalUrl":null,"permalink":"/posts/gitlab/cicd/","section":"Posts","summary":"介紹 # GitLab 是一個開源的 DevOps 平台，提供了版本控制、CI/CD、代碼審查等功能。它的 CI/CD 功能可以幫助開發者自動化軟體開發流程，提高開發效率和質量。","title":"GitLab CICD","type":"posts"},{"content":"","date":"17 September 2023","externalUrl":null,"permalink":"/tags/cni/","section":"Tags","summary":"","title":"cni","type":"tags"},{"content":"","date":"17 September 2023","externalUrl":null,"permalink":"/tags/node/","section":"Tags","summary":"","title":"node","type":"tags"},{"content":"","date":"17 September 2023","externalUrl":null,"permalink":"/tags/pod/","section":"Tags","summary":"","title":"pod","type":"tags"},{"content":" 前言 # 最近公司專案轉雲端架構時，由於服務只能啟用一個 pod 提供線上服務的運作，因此也選擇使用 statefulSet 部署服務，在這過程中發現的問題。\n事件流程 # RD 同仁更新了 code 到 gitLab，但沒有順利完成 CICD。原因是 StatefulSet pod 在關閉時停留在 terminating 狀態。雖然 k8s 有 terminationGracePeriodSeconds 設定，但由於情況特殊，當下的 terminationGracePeriodSeconds 設為 14400 秒，長達四小時。\n因為線上緊急問題，所以針對 terminating pod 採取了強制刪除：kubectl delete pod [pod name] --grace-period=0 --force。之後重新建立的 pod 就會出現以下錯誤訊息 👇\nWarning (combined from similar events): Failed to create pod sandbox: rpo error: code = Unknown desc = failed to setup network for sandbox \u0026#34;14fe0cd3d688aed4ffed4c36ffab1a145230449881bcbe4cac6478a63412b0c*: plugin type=*gke\u0026#34; failed (add): container veth name provided (etho) already exists Google Support # 其實前陣子這個錯誤已經影響到開發和測試環境了。由於這次影響到正式環境，我們把案件等級提升至 P1，並請 Google 協助查找錯誤發生的原因。\n經過 Google 協助分析，這次錯誤的主要原因如下：\n當 pod 進入關閉流程時，由於 terminationGracePeriodSeconds 設置為四小時，pod 仍處於 lifecycle 中 此時使用 kubectl delete pod --force 會導致 pod 雖然消失，但 container 設定仍殘留在 node 上 如果新的 pod 重新在同一顆 node 上啟動，就會造成相同的網路介面設定衝突 雖然改成 Deployment 可以規避此問題，但相對會浪費一組 IP。長久下來一樣會有問題，最重要的還是要讓 pod 完整結束整個 lifecycle，才不會產生後續問題。\n技術細節補充 # Container Network Interface (CNI) 在 Kubernetes 中負責管理 Pod 的網路設定。當 Pod 啟動時，CNI 會為其創建一個虛擬網路介面 (veth pair)，並將其連接到 Pod 的網路命名空間。\n參考資料 # Pods stuck on ContainerCreating after containerd is restarted ","date":"17 September 2023","externalUrl":null,"permalink":"/journals/k8s/veth-name-provided-already-exists/","section":"Journals","summary":"前言 # 最近公司專案轉雲端架構時，由於服務只能啟用一個 pod 提供線上服務的運作，因此也選擇使用 statefulSet 部署服務，在這過程中發現的問題。","title":"解決 Kubernetes Pod 網路衝突：container veth name provided (eth0) already exists","type":"journals"},{"content":"","date":"27 August 2023","externalUrl":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"docker","type":"tags"},{"content":" Dockerfile # Dockerfile 是 Docker 的核心組件之一，用來定義如何建構一個 Docker 映像檔（image）。它是一個文本文件，包含了一系列的指令和參數，這些指令告訴 Docker 如何從基礎映像開始，安裝必要的軟體、配置環境變數、複製檔案等。\n可以保證每次使用相同的 Dockerfile 建構出來的映像檔都是一致的，這對於部署和維護應用程式非常重要。\n這裡要注意 EXPOSE，它用來告訴 Docker 哪些端口需要暴露給外部訪問。但是還是必須要在 docker run 時使用 -p 選項來實際映射端口。\n# 基礎映像，這裡使用 Alpine Linux，如果沒有指定 tag，則默認使用 latest FROM alpine # 設定作者資訊 LABEL maintainer=\u0026#34;bing-wei\u0026#34; # 設定工作目錄 WORKDIR /app # 複製當前目錄下的所有檔案到容器的 /app 目錄 COPY . . # 安裝必要的套件 RUN apk add --no-cache python3 py3-pip # 安裝 Python 相依套件 RUN pip3 install -r requirements.txt # 設定環境變數 ENV PYTHONUNBUFFERED=1 # 暴露容器的 8000 端口 EXPOSE 8000 # 設定容器啟動時執行的命令 CMD [\u0026#34;python3\u0026#34;, \u0026#34;app.py\u0026#34;] 多階段建構 # 在實際的生產環境中，通常會使用多階段建構（multi-stage builds）來減少最終映像檔的大小，這樣可以在開發階段安裝所有必要的工具和依賴，但在最終映像檔中只保留運行應用程式所需的部分。\n# 第一階段：建構階段 FROM node:14 AS build WORKDIR /app COPY package*.json ./ RUN npm install COPY . . RUN npm run build # 第二階段：運行階段 FROM nginx:alpine COPY --from=build /app/dist /usr/share/nginx/html EXPOSE 80 CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] 安全性考量 # 在撰寫 Dockerfile 時，還需要考慮安全性問題，例如：\n使用官方的基礎映像，這樣可以減少安全漏洞的風險。 避免使用 latest 標籤，因為這樣會導致映像檔在不同時間點可能有不同的內容，應該使用具體的版本號。 建立帳號和設定權限，避免使用 root 帳號運行應用程式。 # 使用官方的 Node.js 映像 FROM node:14 # 建立一個非 root 使用者 RUN useradd -m appuser # 切換到非 root 使用者 USER appuser # 設定工作目錄 WORKDIR /home/appuser/app # 複製 package.json 和 package-lock.json COPY package*.json ./ # 安裝相依套件 RUN npm install --only=production # 複製應用程式檔案 COPY --chown=appuser:appuser . . # 暴露應用程式端口 EXPOSE 3000 # 設定容器啟動命令 CMD [\u0026#34;npm\u0026#34;, \u0026#34;start\u0026#34;] ","date":"27 August 2023","externalUrl":null,"permalink":"/posts/dockerfile/","section":"Posts","summary":"Dockerfile # Dockerfile 是 Docker 的核心組件之一，用來定義如何建構一個 Docker 映像檔（image）。它是一個文本文件，包含了一系列的指令和參數，這些指令告訴 Docker 如何從基礎映像開始，安裝必要的軟體、配置環境變數、複製檔案等。","title":"Dockerfile","type":"posts"},{"content":"","date":"19 August 2023","externalUrl":null,"permalink":"/tags/ingress/","section":"Tags","summary":"","title":"ingress","type":"tags"},{"content":"隨著系統架構日益複雜，可能需要使用多個 Ingress 來分離不同服務的流量管理。這樣做的好處是當某個 Ingress 發生異常時，不會影響到所有服務的運作。\n不過缺點是每個 Ingress 都需要使用靜態 IP，在 GCP 等雲端服務中，每個保留的靜態 IP 都會增加成本。\n我們可以使用 Ingress Controller 來統一管理所有的 Ingress 資源，只使用一個或少數幾個靜態 IP，這樣可以降低成本並簡化管理。\n上一篇文章中有提到 ingress，如果想瞭解 ingress 可以先參考或預習這篇文章。\nIngress 6 August 2023\u0026middot;1 分鐘 kubernetes ingress Ingress Nginx Controller # Ingress Nginx Controller 結合了 Ingress 的簡潔並支援 Nginx 相關的擴充功能，讓我們能更好的管理所有的 ingress。\nNginx # 高效能的 webServer 遠勝傳統 apache server 的資源與效能 大量的模組與擴充功能 充足的安全性功能 輕量 容易水平擴展 Ingress # Service 連接 LoadBalance 設定 SSL/TLS 終端 虛擬主機設定 安裝 # helm upgrade --install ingress-nginx ingress-nginx \\ --repo https://kubernetes.github.io/ingress-nginx \\ --namespace ingress-nginx --create-namespace 實際範例 # 基本 Ingress 設定 # apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: example-ingress annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: example.com http: paths: - path: / pathType: Prefix backend: service: name: example-service port: number: 80 進階設定範例 # apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: advanced-ingress annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; nginx.ingress.kubernetes.io/ssl-redirect: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/rate-limit: \u0026#34;10\u0026#34; cert-manager.io/cluster-issuer: \u0026#34;letsencrypt-prod\u0026#34; spec: tls: - hosts: - secure.example.com secretName: example-tls rules: - host: secure.example.com http: paths: - path: /api pathType: Prefix backend: service: name: api-service port: number: 8080 graph LR ingress1(Ingress-1) ingress2(Ingress-2) ingress3(Ingress-3) subgraph Ingress Nginx Controller controller(Controller) admission(Admission Webhook) nginx(Nginx) controller --\u003e|配置| nginx controller --\u003e|驗證| admission end ingress1 --\u003e|管理| controller ingress2 --\u003e|管理| controller ingress3 --\u003e|管理| controller 參考資料 # 使用 Helm 來安裝 Ingress Controller ingress-nginx ","date":"19 August 2023","externalUrl":null,"permalink":"/posts/k8s/ingress-nginx/","section":"Posts","summary":"隨著系統架構日益複雜，可能需要使用多個 Ingress 來分離不同服務的流量管理。這樣做的好處是當某個 Ingress 發生異常時，不會影響到所有服務的運作。","title":"Ingress Nginx Controller","type":"posts"},{"content":" 介紹 # ingress 是 k8s 給 service 提供外部訪問的 URL、SSL、路由等功能。 可以理解為 nginx 或 traefik 等的代理工具。允許透過某個 URL 進入對應的 service，且碰到某些 route 能夠 proxy pass 到其他 service。\n⭐ ingress 又分成外部負載平衡、內部負載平衡。兩者在使用設定上有一些小小的不同。\n外部負載平衡 # 通常對外部的負載平衡入口，都會設置防火牆或是白名單管理但是 k8s ingress 卻不能直接實現。需要透過一些 gcp 元件功能，來設置防火牆或白名單管理。\ncloud armor # armor 的翻譯是指盔甲的意思，這是由 gcp 提供的服務，具有分散式阻斷攻擊(DDos)防護機制、網路應用程式防火牆。可以搭配 loadBalance、Cloud CDN 來強化網路安全服務。\n那麼我們要如何將 cloud armor 應用在我們 ingress 上面呢？接著看下去 😎\n首先我們可以先找官方的使用手冊 Configuring Ingress using the default controller 接著將重點放在 backendConfig 設定上面，前面的介紹已經知道 ingress 負責依據路由規則代理轉發。文件上 backenConfig 與 service 可以耦合。使用 backendConfig 的設定。\nBackendConfig and FrontendConfig overview 詳細想知道 backendConfig 設定範例可以在文件連接內找到 👉 Configuring Ingress features through BackendConfig parameters 接著下面繪製一張外部負載平衡的架構圖。可以看到下圖中 114.1.46.157 的客戶端在訪問的時候被 ingress 給擋下了。\n原因是 ingress 後端 service 有耦合 backendConfig，透過 backendConfig 使用了 cloud armor。\nflowchart LR cli1(client 34.xxx.xxx.xxx) cli2(client 114.1.46.157) cli1 --\u003e ing cli2 --x ing subgraph k8s-cluster ing((ingress)) svc-web(web-service) svc-api(api-service) df-backend(backend-config) armor{{cloud-armor}} ing --\u003e svc-web ing --\u003e svc-api armor -.- df-backend df-backend -.-\u003e svc-web df-backend -.-\u003e svc-api end 內部負載平衡 # 講完了外部負載平衡，接著講 ingress 擔任內部負載平衡時，需要注意的事項。\nNetwork endpoint group # 什麼是 NEG ?\nNEG 是指 Network endpoint group，是一種配置。意思是指定一組後端 endpoint 或 service，借助NEG，Google Cloud 負載均衡器可以為基於 GCE 的工作負載、無服務器工作負載和容器化工作負載提供服務。可以更精細的將流量分配到負載均衡器的後端。\n使用時，對後端的 service 必須要使用 NEG，詳細想要了解可以參考 👉 Network endpoint groups overview ","date":"6 August 2023","externalUrl":null,"permalink":"/posts/k8s/ingress/","section":"Posts","summary":"介紹 # ingress 是 k8s 給 service 提供外部訪問的 URL、SSL、路由等功能。 可以理解為 nginx 或 traefik 等的代理工具。允許透過某個 URL 進入對應的 service，且碰到某些 route 能夠 proxy pass 到其他 service。","title":"Ingress","type":"posts"},{"content":" 匯出/匯入 # export 和 save 都是用來匯出 docker 的映像檔，但是有些不同之處：\nexport 可以匯出在容器中已變更的設定，例如安裝的軟體或修改的配置檔案。 save 單純匯出映像檔，不包含在容器中已變更的設定。 在使用上要注意，如果映像檔使用 export 匯出，則需要使用 import 匯入；如果使用 save 匯出，則需要使用 load 匯入。\ndocker export image \u0026gt; filename.tar docker import \u0026lt; filename.tar docker save image \u0026gt; filename.tar docker load \u0026lt; filename.tar 與容器進行交互 # 通常我們會使用 docker run 指令來啟動容器，如果需要與容器進行交互，可以使用以下選項：\ndocker run -it image /bin/bash 如果要退出容器，可以使用 exit 或 ctrl+d。\n查看容器 # 如果要查看正在運行的容器，可以使用以下指令 docker ps，如果要查看所有的容器（包括已停止的容器），可以使用 docker ps -a。\ndocker ps -a 查看容器內的標準輸出 # 如果要查看容器內的標準輸出，可以使用 docker logs ，但是通常我們會持續的觀察容器的標準輸出，因此可以使用 -f 選項來持續查看。\ndocker logs -f container 查看容器啟動進程 # docker top container 容器資源使用狀況 # docker stats 清理技巧 # ⭐ prune操作是批量刪除類的危險操作，使用 y 確認。不想要輸入可以添加 -f，慎用!\n清除所有停止運行的容器 # docker container prune 清理未使用的映像檔 # docker image prune 清理所有無用的卷 # docker volume prune 參考 # 清理 Docker 的 container，image 與 volume ","date":"30 July 2023","externalUrl":null,"permalink":"/posts/docker/","section":"Posts","summary":"匯出/匯入 # export 和 save 都是用來匯出 docker 的映像檔，但是有些不同之處：","title":"Docker","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/tags/session/","section":"Tags","summary":"","title":"session","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/sticky-session/","section":"Tags","summary":"","title":"sticky session","type":"tags"},{"content":" 前言 # 近期公司有 socket.io 的服務需要可以擴展，由於是聊天室服務，有實時的雙向通信需求（如聊天室、遊戲、實時協作工具）。保持用戶端保持與同一服務器的連接可以：\n確保消息順序一致性 減少連接重建的開銷 維護實時狀態同步 實作 # 建立 backendConfig 設定 sessionAffinity。 apiVersion: cloud.google.com/v1 kind: BackendConfig metadata: name: socket-backendconfig spec: sessionAffinity: affinityType: \u0026#34;GENERATED_COOKIE\u0026#34; # Options: NONE, CLIENT_IP, GENERATED_COOKIE affinityCookieTtlSec: 50 在 kubernetes 的 serivce 中，要先設定 network endpoint group (NEG)，設定的方式有兩種：\n獨立創建 NEG，不管有沒有套用 google load balancer 都會創建一個 NEG。 apiVersion: v1 kind: Service metadata: name: socket-service annotations: cloud.google.com/neg: \u0026#39;{\u0026#34;exposed_ports\u0026#34;: {\u0026#34;80\u0026#34;:{\u0026#34;name\u0026#34;: \u0026#34;NEG_NAME\u0026#34;}}}\u0026#39; cloud.google.com/backend-config: \u0026#39;{\u0026#34;default\u0026#34;: \u0026#34;socket-backendconfig\u0026#34;}\u0026#39; spec: selector: app: socket-app ports: - protocol: TCP port: 80 targetPort: 3000 type: ClusterIP sessionAffinity: ClientIP 依賴於 google load balancer 的自動創建 NEG。 apiVersion: v1 kind: Service metadata: name: socket-service annotations: cloud.google.com/neg: \u0026#39;{\u0026#34;exposed_ports\u0026#34;: {\u0026#34;80\u0026#34;:{\u0026#34;name\u0026#34;: \u0026#34;NEG_NAME\u0026#34;}}}\u0026#39; cloud.google.com/backend-config: \u0026#39;{\u0026#34;default\u0026#34;: \u0026#34;socket-backendconfig\u0026#34;}\u0026#39; spec: selector: app: socket-app ports: - protocol: TCP port: 80 targetPort: 3000 type: ClusterIP sessionAffinity: ClientIP 建立 ingress\n這邊要特別注意的是，這次實作使用的是 GCP 的 NEG，也是要搭配 GCP 的 load balancer 使用。才能達到這次實作的需求。\n因為有使用自己在 github 開源專案建立的 ingress-nginx ，進行串接。但是在前端檢查的時候，發現沒有 GCLB 這個 cookie。\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: socket-ingress annotations: kubernetes.io/ingress.class: \u0026#34;gce\u0026#34; spec: rules: - host: socket.example.com http: paths: - path: / pathType: Prefix backend: service: name: socket-service port: number: 80 都建立完成以後，可以透過以下方式進行測試\ncurl -i -X GET http://socket.example.com 問題反思 # 這次的實作過程中，雖然這種方式看似可以保持客戶端與同一伺服器的連結。但因為是聊天室服務，可能還是會有一些問題需要注意：\n⭐️ session 親和性只是盡可能保持連接在同一伺服器，但無法保證 100% 的穩定性。如果第一次連線是連到 A 伺服器，之後因為某些原因（如伺服器重啟、負載均衡策略變更等）可能會被導向 B 伺服器，這樣就會導致用戶體驗不一致。\n下面是跟一位同仁討論之後紀錄下來的簡易架構圖：\nclient 端在一開始連線的時候，就是不同的 socket_server client 端同時也會透過 api 對 MQ 或者 Pub/Sub 進行訊息的發送。 server 端可以依據 channel 進行訊息的路由。同步給在不同 socket_server 的 client。 flowchart client01 client02 MQ/Pub_Sub socket_server01 socket_server02 client01 --channel,msg--\u003e MQ/Pub_Sub client02 --channel,msg--\u003e MQ/Pub_Sub MQ/Pub_Sub --\u003e socket_server01 MQ/Pub_Sub --\u003e socket_server02 socket_server01 \u003c--\u003e client01 socket_server02 \u003c--\u003e client02 參考資料 # 在 GCP/GKE 的 Ingress 設定 sticky session 透過獨立的區域 NEG 使用容器原生負載平衡 ","externalUrl":null,"permalink":"/journals/k8s/sticky-session/","section":"Journals","summary":"前言 # 近期公司有 socket.","title":"sticky session in kubernetes","type":"journals"}]