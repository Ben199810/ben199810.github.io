[{"content":"","date":"2 July 2025","externalUrl":null,"permalink":"/tags/autoscaling/","section":"Tags","summary":"","title":"autoscaling","type":"tags"},{"content":"","date":"2 July 2025","externalUrl":null,"permalink":"/","section":"bing-wei's Blog","summary":"","title":"bing-wei's Blog","type":"page"},{"content":"","date":"2 July 2025","externalUrl":null,"permalink":"/tags/helm/","section":"Tags","summary":"","title":"helm","type":"tags"},{"content":"","date":"2 July 2025","externalUrl":null,"permalink":"/journals/","section":"Journals","summary":"","title":"Journals","type":"journals"},{"content":"","date":"2 July 2025","externalUrl":null,"permalink":"/tags/k8s/","section":"Tags","summary":"","title":"k8s","type":"tags"},{"content":"最近在升級 helm release 時遇到了一個錯誤，提示 \u0026ldquo;no matches for kind HorizontalPodAutoscaler in version autoscaling/v2beta2\u0026rdquo;。\n原因是因為 Kubernetes 的 API 版本變更，導致舊的資源定義不再匹配新的 API 版本。\n解決方法 # 因為是正式站環境的 log 服務，log 的收集對 RD 在排查問題時非常重要。如果刪除後再重新部署，會導致 log 的丟失。\nHelm3 支援一款插件工具 mapkubeapis，可以將有使用已棄用或已刪除的 API 的 Helm Release metadata 更新為有受支援的 API。\nhelm mapkubeapis -n kube_namespace releaseName 參考資料 # Helm upgrade failure due to deprecated or removed Kubernetes API ","date":"2 July 2025","externalUrl":null,"permalink":"/journals/k8s/kube-object-no-match-for-kind-version/","section":"Journals","summary":"最近在升級 helm release 時遇到了一個錯誤，提示 \u0026ldquo;no matches for kind HorizontalPodAutoscaler in version autoscaling/v2beta2\u0026rdquo;。","title":"Kube Object No Match for Kind Version","type":"journals"},{"content":"","date":"2 July 2025","externalUrl":null,"permalink":"/tags/mapkubeapis/","section":"Tags","summary":"","title":"mapkubeapis","type":"tags"},{"content":"","date":"2 July 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"2 July 2025","externalUrl":null,"permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes","type":"tags"},{"content":" 介紹 # PersistentVolume (PV) 是 Kubernetes 中的一種資源，用於提供持久化存儲。它是集群級別的存儲資源，與 Pod 的生命週期無關。PV 可以被多個 Pod 共享，並且可以在 Pod 重啟或重新調度時保持數據不變。\n管理員可以事先創建 PV，並將其與特定的存儲系統（如 NFS、iSCSI、雲存儲等）綁定，或使用動態存儲類別（StorageClass）來自動創建 PV。\nPersistentVolumeClaim (PVC) 是用戶請求存儲的方式。概念與 Pod 請求資源類似。用戶可以創建 PVC，指定所需的存儲大小和存取模式 (如 ReadWriteOnce、ReadOnlyMany 等)。Kubernetes 會根據 PVC 的要求來匹配合適的 PV。\n存取模式 # ReadWriteOnce (RWO)：卷只能被單個節點以讀寫方式掛載。 ReadOnlyMany (ROX)：卷可以被多個節點以只讀方式掛載。 ReadWriteMany (RWX)：卷可以被多個節點以讀寫方式掛載。 PersistentVolume 與 PersistentVolumeClaim # graph pod(Pod) volume(Volume) PVC(PersistentVolumeClaim) PV(PersistentVolume) storageClass(StorageClass) storageProvider(StorageProvider) storage(Physical Storage) pod --\u003e|使用| volume --\u003e PVC PVC --\u003e|靜態綁定| PV PVC --\u003e|動態綁定| storageClass PV --\u003e|提供存儲| storage storageClass --\u003e|配置存儲| storageProvider --\u003e|實際存儲| storage 如果 PVC storageClassName 屬性為 \u0026quot;\u0026quot;，則表示使用靜態綁定。這意味著 PVC 自身禁止使用動態製備的卷。\n參考 # [Kubernetes / K8s] PV/ PVC 儲存大小事交給PV/PVC管理 持久卷 ","date":"2 July 2025","externalUrl":null,"permalink":"/posts/k8s/storage/","section":"Posts","summary":"介紹 # PersistentVolume (PV) 是 Kubernetes 中的一種資源，用於提供持久化存儲。它是集群級別的存儲資源，與 Pod 的生命週期無關。PV 可以被多個 Pod 共享，並且可以在 Pod 重啟或重新調度時保持數據不變。","title":"PersistentVolume","type":"posts"},{"content":"","date":"2 July 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"2 July 2025","externalUrl":null,"permalink":"/tags/storage/","section":"Tags","summary":"","title":"Storage","type":"tags"},{"content":" 介紹 # 每個 StorageClass 都包含 provisioner、parameters、reclaimPolicy 這些字段會在 StorageClass 需要動態配置 PersistentVolume 以滿足 PersistentVolumeClaim (PVC) 使用到。\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: low-latency provisioner: csi-driver.example-vendor.example reclaimPolicy: Retain # 默认值是 Delete allowVolumeExpansion: true mountOptions: - discard # 这可能会在块存储层启用 UNMAP/TRIM volumeBindingMode: WaitForFirstConsumer parameters: guaranteedReadWriteLatency: \u0026#34;true\u0026#34; # 这是服务提供商特定的 磁碟區插件 # 每個 StorageClass 都會有 provisioner ，用來決定使用哪個磁碟區插件。(必要)\n卷插件 內置製備器 配置範例 AzureFile ✓ Azure File CephFS - - FC - - FlexVolume - - iSCSI - - Local - Local NFS - NFS PortworxVolume ✓ Portworx Volume RBD ✓ Ceph RBD VsphereVolume ✓ vSphere 回收策略 # StorageClass 動態建立的 PersistentVolume 會在 reclaimPolicy 指定回收策略，可以是 Delete 或 Retain 。預設是 Delete 。\n擴展 # PersistentVolume 可以配置為可擴充，允許透過 PVC 物件來調整磁碟區大小，申請一個新的、更大的儲存容量。當 allowVolumeExpansion 定義為 true 時，下列類型的磁碟區支援擴充。\n卷類型 卷擴展的Kubernetes 版本要求 Azure File 1.11 CSI 1.24 FlexVolume 1.13 Portworx 1.11 rbd 1.11 掛載選項 # 由 StorageClass 動態建立的 PersistentVolume 將使用類別中 mountOptions指定的掛載選項。\n如果磁碟區插件不支援掛載選項，卻指定了掛載選項，則製備操作會失敗。 掛載選項在 StorageClass 和 PV 上都不會做驗證。如果其中一個掛載選項無效，那麼這個PV 掛載作業就會失敗。\n參考 # 動態卷製備 ","date":"1 July 2025","externalUrl":null,"permalink":"/posts/k8s/storageclass/","section":"Posts","summary":"介紹 # 每個 StorageClass 都包含 provisioner、parameters、reclaimPolicy 這些字段會在 StorageClass 需要動態配置 PersistentVolume 以滿足 PersistentVolumeClaim (PVC) 使用到。","title":"StorageClass","type":"posts"},{"content":"","date":"24 September 2023","externalUrl":null,"permalink":"/tags/m1/","section":"Tags","summary":"","title":"m1","type":"tags"},{"content":"","date":"24 September 2023","externalUrl":null,"permalink":"/tags/m2/","section":"Tags","summary":"","title":"m2","type":"tags"},{"content":"","date":"24 September 2023","externalUrl":null,"permalink":"/tags/mac/","section":"Tags","summary":"","title":"mac","type":"tags"},{"content":"","date":"24 September 2023","externalUrl":null,"permalink":"/tags/minikube/","section":"Tags","summary":"","title":"minikube","type":"tags"},{"content":" 介紹 # minikube 是一個輕量級的 Kubernetes 環境，主要用於本地開發和測試。它可以在本地機器上啟動一個單節點的 Kubernetes 集群，讓開發者可以快速地部署和測試應用程式。\nminikube 支援多種虛擬化技術，包括 VirtualBox、VMware、KVM、Hyper-V 等等。它可以在 Windows、macOS 和 Linux 上運行。\nQemu # Qemu 是一個開源的虛擬化軟體，可以模擬多種硬體架構，包括 x86、ARM、MIPS 等等。它可以用於虛擬化和模擬不同的作業系統和應用程式。\nQemu 的主要功能包括：\n使用者程式模擬：QEMU 能夠將為一個平台編譯的二進位檔案運行在另一個不同的平台。 系統虛擬化模擬：QEMU 能夠模擬一個完整的系統虛擬機，該虛擬機有自己的虛擬CPU、晶片組、虛擬記憶體以及各種虛擬外部設備，能夠為虛擬機中運行的作業系統和應用軟體呈現出與實體電腦完全一致的硬體視圖。QEMU能夠模擬 x86、ARM、MIPS、PPC 等多個平台。 Qemu 在 macOS 上的使用主要是為了支援 M 系列晶片（如 M1、M2）的虛擬化，因為這些晶片使用 ARM 架構，而許多傳統的虛擬化技術（如 VirtualBox）並不支援 ARM 架構。\n安裝 # 在 macOS 上安裝 minikube 和 Qemu，可以使用 Homebrew 來簡化安裝過程。 首先，確保你已經安裝了 Homebrew。如果還沒有安裝，可以參考 Homebrew 官方網站 進行安裝。\nbrew install minikube brew install qemu 接著，為了讓 minikube 使用 Qemu 作為虛擬化驅動程式，我們需要安裝 socket_vmnet。這是一個用於 macOS 的 Qemu 虛擬化網路驅動程式。\nbrew install socket_vmnet brew tap homebrew/services HOMEBREW=$(which brew) \u0026amp;\u0026amp; sudo ${HOMEBREW} services start socket_vmnet 啟動 Minikube # 在安裝完成後，可以使用以下命令啟動 minikube，並指定使用 Qemu 作為虛擬化驅動程式。\nminikube start --driver qemu --network socket_vmnet 參考資料 # How to Setup Minikube on MAC M1/M2 ","date":"24 September 2023","externalUrl":null,"permalink":"/posts/minikube/","section":"Posts","summary":"介紹 # minikube 是一個輕量級的 Kubernetes 環境，主要用於本地開發和測試。它可以在本地機器上啟動一個單節點的 Kubernetes 集群，讓開發者可以快速地部署和測試應用程式。","title":"Minikube 環境與安裝 (Mac M1/M2)","type":"posts"},{"content":"","date":"22 September 2023","externalUrl":null,"permalink":"/tags/cicd/","section":"Tags","summary":"","title":"cicd","type":"tags"},{"content":"","date":"22 September 2023","externalUrl":null,"permalink":"/tags/gitlab/","section":"Tags","summary":"","title":"gitlab","type":"tags"},{"content":" 介紹 # GitLab 是一個開源的 DevOps 平台，提供了版本控制、CI/CD、代碼審查等功能。它的 CI/CD 功能可以幫助開發者自動化軟體開發流程，提高開發效率和質量。\nGitLab 的 CI/CD 功能基於 GitLab Runner，允許開發者定義自動化的工作流程，從代碼提交到部署生產環境都可以自動化完成。\nGitLab CI/CD 的配置文件是 .gitlab-ci.yml，這個文件定義了 CI/CD 的工作流程，包括各個階段（stages）、任務（jobs）和執行的腳本（scripts）。開發者可以根據自己的需求定義不同的工作流程，並且可以在不同的階段中執行不同的任務。\n基本配置 # 以下是一個簡單的 .gitlab-ci.yml 配置範例，包含了三個階段：test、build 和 deploy。\nstages: - test - build - deploy test: stage: test script: # 定義 test 階段的指令 - echo \u0026#34;Running tests\u0026#34; # 執行單元測試、程式碼品質檢查等 - npm test - npm run lint build: stage: build script: # 定義 build 階段的指令 - echo \u0026#34;Building the application\u0026#34; - docker build -t $IMAGE_NAME:$TAG . # 只有當測試通過後才執行 build needs: [\u0026#34;test\u0026#34;] deploy: stage: deploy script: # 定義 deploy 階段的指令 - echo \u0026#34;Deploying to production\u0026#34; # 在實際情況下，這裡可以是部署到 Kubernetes、AWS、GCP 等的相應指令 # 也可以使用 Helm 進行部署 # 只有當 build 成功後才執行 deploy needs: [\u0026#34;build\u0026#34;] 進階配置 # 在生產環境中，通常需要更複雜的配置，例如：配置環境變數、建構 mysql 資料庫、redis 等服務。\nstages: - test - build - deploy test: stage: test services: - name: mysql:5.7 alias: db - redis:latest alias: redis variables: MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD} MYSQL_DATABASE: ${MYSQL_DATABASE} REDIS_PASSWORD: ${REDIS_PASSWORD} script: - echo \u0026#34;Running tests\u0026#34; - npm test - npm run lint build: stage: build script: - echo \u0026#34;Building the application\u0026#34; - docker build -t $IMAGE_NAME:$TAG . needs: [\u0026#34;test\u0026#34;] deploy: stage: deploy script: - echo \u0026#34;Deploying to production\u0026#34; needs: [\u0026#34;build\u0026#34;] parallel:matrix # ⚠️ 對於 runner 的配置，必須要有多台 runner 或者配置單台 runner 配置支援同時執行多個任務。\n⚠️ 矩陣的排列數不能超過 200\nparallel 可以在單一的 pipeline 中同時執行多個任務，這對於需要在多個環境或配置下運行相同任務的情況非常有用。\n如果需要在不同的 Node.js 版本和環境下運行測試，可以使用 parallel:matrix 來定義一個矩陣，這樣可以在不同的配置下同時運行相同的任務。\ntest: stage: test script: - echo \u0026#34;Running tests\u0026#34; parallel: matrix: - NODE_VERSION: [10, 12, 14] - ENV: [development, production] 參考 # Parallel ","date":"22 September 2023","externalUrl":null,"permalink":"/posts/gitlab/cicd/","section":"Posts","summary":"介紹 # GitLab 是一個開源的 DevOps 平台，提供了版本控制、CI/CD、代碼審查等功能。它的 CI/CD 功能可以幫助開發者自動化軟體開發流程，提高開發效率和質量。","title":"GitLab CICD","type":"posts"},{"content":"","date":"17 September 2023","externalUrl":null,"permalink":"/tags/cni/","section":"Tags","summary":"","title":"cni","type":"tags"},{"content":"","date":"17 September 2023","externalUrl":null,"permalink":"/tags/node/","section":"Tags","summary":"","title":"node","type":"tags"},{"content":"","date":"17 September 2023","externalUrl":null,"permalink":"/tags/pod/","section":"Tags","summary":"","title":"pod","type":"tags"},{"content":" 前言 # 最近公司專案轉雲端架構時，由於服務只能啟用一個 pod 提供線上服務的運作，因此也選擇使用 statefulSet 部署服務，在這過程中發現的問題。\n事件流程 # RD 同仁更新了 code 到 gitLab，但沒有順利完成 CICD。原因是 StatefulSet pod 在關閉時停留在 terminating 狀態。雖然 k8s 有 terminationGracePeriodSeconds 設定，但由於情況特殊，當下的 terminationGracePeriodSeconds 設為 14400 秒，長達四小時。\n因為線上緊急問題，所以針對 terminating pod 採取了強制刪除：kubectl delete pod [pod name] --grace-period=0 --force。之後重新建立的 pod 就會出現以下錯誤訊息 👇\nWarning (combined from similar events): Failed to create pod sandbox: rpo error: code = Unknown desc = failed to setup network for sandbox \u0026#34;14fe0cd3d688aed4ffed4c36ffab1a145230449881bcbe4cac6478a63412b0c*: plugin type=*gke\u0026#34; failed (add): container veth name provided (etho) already exists Google Support # 其實前陣子這個錯誤已經影響到開發和測試環境了。由於這次影響到正式環境，我們把案件等級提升至 P1，並請 Google 協助查找錯誤發生的原因。\n經過 Google 協助分析，這次錯誤的主要原因如下：\n當 pod 進入關閉流程時，由於 terminationGracePeriodSeconds 設置為四小時，pod 仍處於 lifecycle 中 此時使用 kubectl delete pod --force 會導致 pod 雖然消失，但 container 設定仍殘留在 node 上 如果新的 pod 重新在同一顆 node 上啟動，就會造成相同的網路介面設定衝突 雖然改成 Deployment 可以規避此問題，但相對會浪費一組 IP。長久下來一樣會有問題，最重要的還是要讓 pod 完整結束整個 lifecycle，才不會產生後續問題。\n技術細節補充 # Container Network Interface (CNI) 在 Kubernetes 中負責管理 Pod 的網路設定。當 Pod 啟動時，CNI 會為其創建一個虛擬網路介面 (veth pair)，並將其連接到 Pod 的網路命名空間。\n參考資料 # Pods stuck on ContainerCreating after containerd is restarted ","date":"17 September 2023","externalUrl":null,"permalink":"/journals/k8s/veth-name-provided-already-exists/","section":"Journals","summary":"前言 # 最近公司專案轉雲端架構時，由於服務只能啟用一個 pod 提供線上服務的運作，因此也選擇使用 statefulSet 部署服務，在這過程中發現的問題。","title":"解決 Kubernetes Pod 網路衝突：container veth name provided (eth0) already exists","type":"journals"},{"content":"","date":"27 August 2023","externalUrl":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"docker","type":"tags"},{"content":" Dockerfile # Dockerfile 是 Docker 的核心組件之一，用來定義如何建構一個 Docker 映像檔（image）。它是一個文本文件，包含了一系列的指令和參數，這些指令告訴 Docker 如何從基礎映像開始，安裝必要的軟體、配置環境變數、複製檔案等。\n可以保證每次使用相同的 Dockerfile 建構出來的映像檔都是一致的，這對於部署和維護應用程式非常重要。\n這裡要注意 EXPOSE，它用來告訴 Docker 哪些端口需要暴露給外部訪問。但是還是必須要在 docker run 時使用 -p 選項來實際映射端口。\n# 基礎映像，這裡使用 Alpine Linux，如果沒有指定 tag，則默認使用 latest FROM alpine # 設定作者資訊 LABEL maintainer=\u0026#34;bing-wei\u0026#34; # 設定工作目錄 WORKDIR /app # 複製當前目錄下的所有檔案到容器的 /app 目錄 COPY . . # 安裝必要的套件 RUN apk add --no-cache python3 py3-pip # 安裝 Python 相依套件 RUN pip3 install -r requirements.txt # 設定環境變數 ENV PYTHONUNBUFFERED=1 # 暴露容器的 8000 端口 EXPOSE 8000 # 設定容器啟動時執行的命令 CMD [\u0026#34;python3\u0026#34;, \u0026#34;app.py\u0026#34;] 多階段建構 # 在實際的生產環境中，通常會使用多階段建構（multi-stage builds）來減少最終映像檔的大小，這樣可以在開發階段安裝所有必要的工具和依賴，但在最終映像檔中只保留運行應用程式所需的部分。\n# 第一階段：建構階段 FROM node:14 AS build WORKDIR /app COPY package*.json ./ RUN npm install COPY . . RUN npm run build # 第二階段：運行階段 FROM nginx:alpine COPY --from=build /app/dist /usr/share/nginx/html EXPOSE 80 CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] 安全性考量 # 在撰寫 Dockerfile 時，還需要考慮安全性問題，例如：\n使用官方的基礎映像，這樣可以減少安全漏洞的風險。 避免使用 latest 標籤，因為這樣會導致映像檔在不同時間點可能有不同的內容，應該使用具體的版本號。 建立帳號和設定權限，避免使用 root 帳號運行應用程式。 # 使用官方的 Node.js 映像 FROM node:14 # 建立一個非 root 使用者 RUN useradd -m appuser # 切換到非 root 使用者 USER appuser # 設定工作目錄 WORKDIR /home/appuser/app # 複製 package.json 和 package-lock.json COPY package*.json ./ # 安裝相依套件 RUN npm install --only=production # 複製應用程式檔案 COPY --chown=appuser:appuser . . # 暴露應用程式端口 EXPOSE 3000 # 設定容器啟動命令 CMD [\u0026#34;npm\u0026#34;, \u0026#34;start\u0026#34;] ","date":"27 August 2023","externalUrl":null,"permalink":"/posts/dockerfile/","section":"Posts","summary":"Dockerfile # Dockerfile 是 Docker 的核心組件之一，用來定義如何建構一個 Docker 映像檔（image）。它是一個文本文件，包含了一系列的指令和參數，這些指令告訴 Docker 如何從基礎映像開始，安裝必要的軟體、配置環境變數、複製檔案等。","title":"Dockerfile","type":"posts"},{"content":"","date":"19 August 2023","externalUrl":null,"permalink":"/tags/ingress/","section":"Tags","summary":"","title":"ingress","type":"tags"},{"content":"隨著系統架構日益複雜，可能需要使用多個 Ingress 來分離不同服務的流量管理。這樣做的好處是當某個 Ingress 發生異常時，不會影響到所有服務的運作。\n不過缺點是每個 Ingress 都需要使用靜態 IP，在 GCP 等雲端服務中，每個保留的靜態 IP 都會增加成本。\n我們可以使用 Ingress Controller 來統一管理所有的 Ingress 資源，只使用一個或少數幾個靜態 IP，這樣可以降低成本並簡化管理。\n上一篇文章中有提到 ingress，如果想瞭解 ingress 可以先參考或預習這篇文章。\nIngress 6 August 2023\u0026middot;1 分鐘 k8s ingress Ingress Nginx Controller # Ingress Nginx Controller 結合了 Ingress 的簡潔並支援 Nginx 相關的擴充功能，讓我們能更好的管理所有的 ingress。\nNginx # 高效能的 webServer 遠勝傳統 apache server 的資源與效能 大量的模組與擴充功能 充足的安全性功能 輕量 容易水平擴展 Ingress # Service 連接 LoadBalance 設定 SSL/TLS 終端 虛擬主機設定 安裝 # helm upgrade --install ingress-nginx ingress-nginx \\ --repo https://kubernetes.github.io/ingress-nginx \\ --namespace ingress-nginx --create-namespace 實際範例 # 基本 Ingress 設定 # apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: example-ingress annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: example.com http: paths: - path: / pathType: Prefix backend: service: name: example-service port: number: 80 進階設定範例 # apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: advanced-ingress annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; nginx.ingress.kubernetes.io/ssl-redirect: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/rate-limit: \u0026#34;10\u0026#34; cert-manager.io/cluster-issuer: \u0026#34;letsencrypt-prod\u0026#34; spec: tls: - hosts: - secure.example.com secretName: example-tls rules: - host: secure.example.com http: paths: - path: /api pathType: Prefix backend: service: name: api-service port: number: 8080 graph LR ingress1(Ingress-1) ingress2(Ingress-2) ingress3(Ingress-3) subgraph Ingress Nginx Controller controller(Controller) admission(Admission Webhook) nginx(Nginx) controller --\u003e|配置| nginx controller --\u003e|驗證| admission end ingress1 --\u003e|管理| controller ingress2 --\u003e|管理| controller ingress3 --\u003e|管理| controller 參考資料 # 使用 Helm 來安裝 Ingress Controller ingress-nginx ","date":"19 August 2023","externalUrl":null,"permalink":"/posts/k8s/ingress-nginx/","section":"Posts","summary":"隨著系統架構日益複雜，可能需要使用多個 Ingress 來分離不同服務的流量管理。這樣做的好處是當某個 Ingress 發生異常時，不會影響到所有服務的運作。","title":"Ingress Nginx Controller","type":"posts"},{"content":" 介紹 # ingress 是 k8s 給 service 提供外部訪問的 URL、SSL、路由等功能。 可以理解為 nginx 或 traefik 等的代理工具。允許透過某個 URL 進入對應的 service，且碰到某些 route 能夠 proxy pass 到其他 service。\n⭐ ingress 又分成外部負載平衡、內部負載平衡。兩者在使用設定上有一些小小的不同。\n外部負載平衡 # 通常對外部的負載平衡入口，都會設置防火牆或是白名單管理但是 k8s ingress 卻不能直接實現。需要透過一些 gcp 元件功能，來設置防火牆或白名單管理。\ncloud armor # armor 的翻譯是指盔甲的意思，這是由 gcp 提供的服務，具有分散式阻斷攻擊(DDos)防護機制、網路應用程式防火牆。可以搭配 loadBalance、Cloud CDN 來強化網路安全服務。\n那麼我們要如何將 cloud armor 應用在我們 ingress 上面呢？接著看下去 😎\n首先我們可以先找官方的使用手冊 Configuring Ingress using the default controller 接著將重點放在 backendConfig 設定上面，前面的介紹已經知道 ingress 負責依據路由規則代理轉發。文件上 backenConfig 與 service 可以耦合。使用 backendConfig 的設定。\nBackendConfig and FrontendConfig overview 詳細想知道 backendConfig 設定範例可以在文件連接內找到 👉 Configuring Ingress features through BackendConfig parameters 接著下面繪製一張外部負載平衡的架構圖。可以看到下圖中 114.1.46.157 的客戶端在訪問的時候被 ingress 給擋下了。\n原因是 ingress 後端 service 有耦合 backendConfig，透過 backendConfig 使用了 cloud armor。\nflowchart LR cli1(client 34.xxx.xxx.xxx) cli2(client 114.1.46.157) cli1 --\u003e ing cli2 --x ing subgraph k8s-cluster ing((ingress)) svc-web(web-service) svc-api(api-service) df-backend(backend-config) armor{{cloud-armor}} ing --\u003e svc-web ing --\u003e svc-api armor -.- df-backend df-backend -.-\u003e svc-web df-backend -.-\u003e svc-api end 內部負載平衡 # 講完了外部負載平衡，接著講 ingress 擔任內部負載平衡時，需要注意的事項。\nNetwork endpoint group # 什麼是 NEG ?\nNEG 是指 Network endpoint group，是一種配置。意思是指定一組後端 endpoint 或 service，借助NEG，Google Cloud 負載均衡器可以為基於 GCE 的工作負載、無服務器工作負載和容器化工作負載提供服務。可以更精細的將流量分配到負載均衡器的後端。\n使用時，對後端的 service 必須要使用 NEG，詳細想要了解可以參考 👉 Network endpoint groups overview ","date":"6 August 2023","externalUrl":null,"permalink":"/posts/k8s/ingress/","section":"Posts","summary":"介紹 # ingress 是 k8s 給 service 提供外部訪問的 URL、SSL、路由等功能。 可以理解為 nginx 或 traefik 等的代理工具。允許透過某個 URL 進入對應的 service，且碰到某些 route 能夠 proxy pass 到其他 service。","title":"Ingress","type":"posts"},{"content":" 匯出/匯入 # export 和 save 都是用來匯出 docker 的映像檔，但是有些不同之處：\nexport 可以匯出在容器中已變更的設定，例如安裝的軟體或修改的配置檔案。 save 單純匯出映像檔，不包含在容器中已變更的設定。 在使用上要注意，如果映像檔使用 export 匯出，則需要使用 import 匯入；如果使用 save 匯出，則需要使用 load 匯入。\ndocker export image \u0026gt; filename.tar docker import \u0026lt; filename.tar docker save image \u0026gt; filename.tar docker load \u0026lt; filename.tar 與容器進行交互 # 通常我們會使用 docker run 指令來啟動容器，如果需要與容器進行交互，可以使用以下選項：\ndocker run -it image /bin/bash 如果要退出容器，可以使用 exit 或 ctrl+d。\n查看容器 # 如果要查看正在運行的容器，可以使用以下指令 docker ps，如果要查看所有的容器（包括已停止的容器），可以使用 docker ps -a。\ndocker ps -a 查看容器內的標準輸出 # 如果要查看容器內的標準輸出，可以使用 docker logs ，但是通常我們會持續的觀察容器的標準輸出，因此可以使用 -f 選項來持續查看。\ndocker logs -f container 查看容器啟動進程 # docker top container 容器資源使用狀況 # docker stats 清理技巧 # ⭐ prune操作是批量刪除類的危險操作，使用 y 確認。不想要輸入可以添加 -f，慎用!\n清除所有停止運行的容器 # docker container prune 清理未使用的映像檔 # docker image prune 清理所有無用的卷 # docker volume prune 參考 # 清理 Docker 的 container，image 與 volume ","date":"30 July 2023","externalUrl":null,"permalink":"/posts/docker/","section":"Posts","summary":"匯出/匯入 # export 和 save 都是用來匯出 docker 的映像檔，但是有些不同之處：","title":"Docker","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]